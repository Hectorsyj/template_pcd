{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xHJdORHf1N6"
   },
   "source": [
    "# Recolección y Exploración del Corpus de Noticias\n",
    "\n",
    "## Introducción\n",
    "El análisis de texto es una técnica fundamental en el procesamiento del lenguaje natural (NLP), utilizada para extraer información significativa de grandes volúmenes de datos textuales. En este caso, trabajaremos con un conjunto de noticias provenientes de un archivo de datos en Excel, con el objetivo de realizar una carga, exploración y análisis inicial del corpus de noticias.\n",
    "\n",
    "### Este cuaderno Jupyter se enfocará en:\n",
    "\n",
    "1. Carga y visualización del corpus \n",
    "* Importación del archivo de noticias en un DataFrame de Pandas.\n",
    "* Inspección de la estructura y calidad de los datos.\n",
    "* Identificación de valores nulos o inconsistencias en las noticias.\n",
    "\n",
    "2. Exploración del corpus\n",
    "* Análisis estadístico del conjunto de datos.\n",
    "* Conteo de la cantidad de noticias por categoría.\n",
    "* Cálculo de la longitud promedio de los artículos en términos de palabras.\n",
    "\n",
    "3. Palabras más frecuentes\n",
    "* Extracción de todas las palabras del corpus de noticias.\n",
    "* Conteo de la frecuencia de cada palabra en el conjunto de datos.\n",
    "* Identificación de las palabras más utilizadas en los artículos.\n",
    "\n",
    "## Importancia del Corpus de Noticias\n",
    "El análisis de este corpus nos permitirá identificar tendencias, patrones lingüísticos y relaciones entre palabras, lo cual es útil para aplicaciones como:\n",
    "\n",
    "* Análisis de sentimientos: Determinar la polaridad de las noticias.\n",
    "* Clasificación de noticias: Organizar artículos en categorías relevantes.\n",
    "* Extracción de palabras clave: Identificar términos importantes dentro del corpus.\n",
    "* Modelado de temas: Descubrir patrones y agrupaciones temáticas en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga y Visualización del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLXwaA3PfYCm",
    "outputId": "97fbaa7d-16a7-4d54-b719-6ace091f6e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código descarga dos recursos clave de la biblioteca NLTK (Natural Language Toolkit), que es una de las herramientas más utilizadas para el procesamiento del lenguaje natural (NLP):\n",
    "\n",
    "1. nltk.download('punkt'):\n",
    "* Descarga el paquete Punkt, que contiene un modelo de tokenización preentrenado.\n",
    "* Se utiliza para dividir un texto en oraciones o palabras, facilitando el análisis del corpus.\n",
    "\n",
    "2. nltk.download('stopwords'):\n",
    "* Descarga una lista de stopwords (palabras vacías o de poco significado, como \"el\", \"la\", \"de\", etc.).\n",
    "* Estas palabras suelen eliminarse en tareas de análisis de texto, ya que no aportan mucho significado a nivel de contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09BzO0Byg7IA",
    "outputId": "c4f4365e-c2d3-4fa9-beea-bce746b8263d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14396, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el archivo de datos\n",
    "file_path = r'C:\\Users\\ojito\\Gerencia de Proyectos 2\\Noticias.xlsx'\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Mostrar una vista previa de los datos\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código se encarga de importar bibliotecas, descargar recursos de NLP, cargar el dataset y mostrar su estructura.\n",
    "\n",
    "1. Importación de bibliotecas\n",
    "* pandas: Para la manipulación y análisis del dataset.\n",
    "* nltk.tokenize.word_tokenize: Para dividir el texto en palabras.\n",
    "* string y re: Para manipulación de texto y expresiones regulares.\n",
    "* TfidfVectorizer (Scikit-learn): Para la vectorización de texto usando TF-IDF, una técnica que mide la importancia de cada palabra en un documento.\n",
    "* Word2Vec (Gensim): Para la representación de palabras en vectores numéricos basados en su contexto en el corpus.\n",
    "\n",
    "2. Descarga de recursos de NLTK\n",
    "* punkt: Para tokenización en NLTK.\n",
    "* stopwords: Contiene palabras vacías en varios idiomas.\n",
    "\n",
    "3. Carga del dataset\n",
    "* Carga el archivo de Excel en un DataFrame de Pandas.\n",
    "* Se asume que el script se ejecuta en Google Colab (por el uso de '/content/' como ruta).\n",
    "\n",
    "4. Visualización de la estructura de los datos\n",
    "* shape devuelve la forma del dataset en (filas, columnas), permitiendo conocer el tamaño del corpus de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente con 14396 filas y 6 columnas.\n"
     ]
    }
   ],
   "source": [
    "if data.empty:\n",
    "    print(\"Error: El archivo está vacío o no se pudo cargar.\")\n",
    "else:\n",
    "    print(f\"Dataset cargado correctamente con {data.shape[0]} filas y {data.shape[1]} columnas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las primeras filas para entender la estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "GOfkKxe4ZE5g",
    "outputId": "e0d6c288-fbed-4b2e-a237-f60b9aaba496"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna1</th>\n",
       "      <th>Enlaces</th>\n",
       "      <th>Título</th>\n",
       "      <th>info</th>\n",
       "      <th>contenido</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.eltiempo.com/agresion-contra-un-op...</td>\n",
       "      <td>Operador de grúa quedó inconsciente tras agres...</td>\n",
       "      <td>El conductor de una moto le lanzó el casco y p...</td>\n",
       "      <td>Las autoridades están buscando al conductor de...</td>\n",
       "      <td>colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Usaquén, primera en infracciones por mal parqueo</td>\n",
       "      <td>La localidad ocupa el primer lugar en comparen...</td>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>'Me atracaron y vi un arma que me heló la sang...</td>\n",
       "      <td>Un ciudadano relata cómo cuatro hombres lo rob...</td>\n",
       "      <td>A las 7 de la noche me había quedado de encont...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Escoltas mal estacionados, dolor de cabeza de ...</td>\n",
       "      <td>Las zonas de restaurantes se convierten en par...</td>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Radicado primer proyecto que autorizaría union...</td>\n",
       "      <td>El representante de 'la U', Miguel Gómez, dijo...</td>\n",
       "      <td>“Estamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Columna1                                            Enlaces  \\\n",
       "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
       "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "\n",
       "                                              Título  \\\n",
       "0  Operador de grúa quedó inconsciente tras agres...   \n",
       "1   Usaquén, primera en infracciones por mal parqueo   \n",
       "2  'Me atracaron y vi un arma que me heló la sang...   \n",
       "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
       "4  Radicado primer proyecto que autorizaría union...   \n",
       "\n",
       "                                                info  \\\n",
       "0  El conductor de una moto le lanzó el casco y p...   \n",
       "1  La localidad ocupa el primer lugar en comparen...   \n",
       "2  Un ciudadano relata cómo cuatro hombres lo rob...   \n",
       "3  Las zonas de restaurantes se convierten en par...   \n",
       "4  El representante de 'la U', Miguel Gómez, dijo...   \n",
       "\n",
       "                                           contenido  Etiqueta  \n",
       "0  Las autoridades están buscando al conductor de...  colombia  \n",
       "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
       "2  A las 7 de la noche me había quedado de encont...   archivo  \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
       "4  “Estamos proponiendo la figura de un contrato ...   archivo  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploración del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Cantidad de noticias por etiqueta:\n",
      "Etiqueta\n",
      "archivo                 9187\n",
      "colombia                 934\n",
      "deportes                 727\n",
      "opinion                  532\n",
      "mundo                    446\n",
      "cultura                  430\n",
      "economia                 367\n",
      "justicia                 343\n",
      "bogota                   311\n",
      "vida                     268\n",
      "politica                 252\n",
      "tecnosfera               214\n",
      "salud                    106\n",
      "historias-el-tiempo       57\n",
      "mundial                   47\n",
      "contenido-comercial       34\n",
      "elecciones                33\n",
      "unidad-investigativa      27\n",
      "podcast                   20\n",
      "foro-w                    18\n",
      "bocas                     15\n",
      "carrusel                   8\n",
      "datos                      7\n",
      "lecturas-dominicales       6\n",
      "mas-contenido              4\n",
      "especiales                 3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📊 Longitud promedio de los artículos: 531.6849614208764\n",
      "\n",
      "🔝 Palabras más frecuentes en el corpus:\n",
      "[('de', 496879), ('la', 253580), ('en', 212954), ('que', 190982), ('el', 181023), ('y', 161176), ('a', 135845), ('del', 112492), ('los', 92772), ('con', 85706)]\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de noticias por categoría\n",
    "print(\"\\n📌 Cantidad de noticias por etiqueta:\")\n",
    "print(data[\"Etiqueta\"].value_counts())\n",
    "\n",
    "# Longitud promedio de los artículos (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "data[\"longitud\"] = data[\"contenido\"].dropna().apply(lambda x: len(str(x).split()))\n",
    "print(\"\\n📊 Longitud promedio de los artículos:\", data[\"longitud\"].mean())\n",
    "\n",
    "# Palabras más frecuentes en el corpus (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "from collections import Counter\n",
    "\n",
    "# Concatenar todos los textos en un solo string\n",
    "texto_completo = \" \".join(data[\"contenido\"].dropna())\n",
    "palabras = texto_completo.split()\n",
    "conteo_palabras = Counter(palabras)\n",
    "\n",
    "# Mostrar las 10 palabras más comunes\n",
    "print(\"\\n🔝 Palabras más frecuentes en el corpus:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cantidad de noticias por categoría (Etiqueta)\n",
    "* Se cuenta cuántas noticias hay en cada categoría y se imprime el resultado.\n",
    "  \n",
    "2. Longitud promedio de los artículos (contenido)\n",
    "* Se calcula la cantidad promedio de palabras en los artículos.\n",
    "* Se usa .dropna() para evitar errores con valores nulos.\n",
    "* Se utiliza split() para contar el número de palabras en cada artículo.\n",
    "  \n",
    "3. Palabras más frecuentes en el corpus (contenido)\n",
    "* se concatenan todos los artículos en un solo texto.\n",
    "* Se separan las palabras usando split().\n",
    "* Se utiliza Counter de la biblioteca collections para contar la frecuencia de cada palabra.\n",
    "* Se imprimen las 10 palabras más comunes en el corpus de noticias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Palabras Más Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔝 Palabras más frecuentes en el corpus:\n",
      "[('de', 496879), ('la', 253580), ('en', 212954), ('que', 190982), ('el', 181023), ('y', 161176), ('a', 135845), ('del', 112492), ('los', 92772), ('con', 85706)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Obtener todas las palabras en una sola lista (usando 'contenido' en lugar de 'tokens_sin_stopwords')\n",
    "todas_las_palabras = [word for text in data[\"contenido\"].dropna() for word in text.split()]\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "conteo_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Mostrar las 10 palabras más comunes\n",
    "print(\"\\n🔝 Palabras más frecuentes en el corpus:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código cuenta las palabras más frecuentes en el corpus de noticias utilizando la columna contenido. Aquí está el desglose de lo que hace:\n",
    "\n",
    "1. Obtener todas las palabras en una sola lista\n",
    "* Se itera sobre la columna contenido, asegurando que no haya valores nulos con .dropna().\n",
    "* Cada artículo de noticias se divide en palabras usando .split(), generando una lista de todas las palabras.\n",
    "\n",
    "2. Contar la frecuencia de cada palabra\n",
    "* Se utiliza la clase Counter de la biblioteca collections para contar cuántas veces aparece cada palabra en el conjunto de datos.\n",
    "\n",
    "3. Mostrar las 10 palabras más comunes\n",
    "* Se usa most_common(10) para obtener e imprimir las 10 palabras con mayor frecuencia en el corpus."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
