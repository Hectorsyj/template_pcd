{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xHJdORHf1N6"
   },
   "source": [
    "# Recolecci√≥n y Exploraci√≥n del Corpus de Noticias\n",
    "\n",
    "## Introducci√≥n\n",
    "El an√°lisis de texto es una t√©cnica fundamental en el procesamiento del lenguaje natural (NLP), utilizada para extraer informaci√≥n significativa de grandes vol√∫menes de datos textuales. En este caso, trabajaremos con un conjunto de noticias provenientes de un archivo de datos en Excel, con el objetivo de realizar una carga, exploraci√≥n y an√°lisis inicial del corpus de noticias.\n",
    "\n",
    "### Este cuaderno Jupyter se enfocar√° en:\n",
    "\n",
    "1. Carga y visualizaci√≥n del corpus \n",
    "* Importaci√≥n del archivo de noticias en un DataFrame de Pandas.\n",
    "* Inspecci√≥n de la estructura y calidad de los datos.\n",
    "* Identificaci√≥n de valores nulos o inconsistencias en las noticias.\n",
    "\n",
    "2. Exploraci√≥n del corpus\n",
    "* An√°lisis estad√≠stico del conjunto de datos.\n",
    "* Conteo de la cantidad de noticias por categor√≠a.\n",
    "* C√°lculo de la longitud promedio de los art√≠culos en t√©rminos de palabras.\n",
    "\n",
    "3. Palabras m√°s frecuentes\n",
    "* Extracci√≥n de todas las palabras del corpus de noticias.\n",
    "* Conteo de la frecuencia de cada palabra en el conjunto de datos.\n",
    "* Identificaci√≥n de las palabras m√°s utilizadas en los art√≠culos.\n",
    "\n",
    "## Importancia del Corpus de Noticias\n",
    "El an√°lisis de este corpus nos permitir√° identificar tendencias, patrones ling√º√≠sticos y relaciones entre palabras, lo cual es √∫til para aplicaciones como:\n",
    "\n",
    "* An√°lisis de sentimientos: Determinar la polaridad de las noticias.\n",
    "* Clasificaci√≥n de noticias: Organizar art√≠culos en categor√≠as relevantes.\n",
    "* Extracci√≥n de palabras clave: Identificar t√©rminos importantes dentro del corpus.\n",
    "* Modelado de temas: Descubrir patrones y agrupaciones tem√°ticas en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga y Visualizaci√≥n del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLXwaA3PfYCm",
    "outputId": "97fbaa7d-16a7-4d54-b719-6ace091f6e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de c√≥digo descarga dos recursos clave de la biblioteca NLTK (Natural Language Toolkit), que es una de las herramientas m√°s utilizadas para el procesamiento del lenguaje natural (NLP):\n",
    "\n",
    "1. nltk.download('punkt'):\n",
    "* Descarga el paquete Punkt, que contiene un modelo de tokenizaci√≥n preentrenado.\n",
    "* Se utiliza para dividir un texto en oraciones o palabras, facilitando el an√°lisis del corpus.\n",
    "\n",
    "2. nltk.download('stopwords'):\n",
    "* Descarga una lista de stopwords (palabras vac√≠as o de poco significado, como \"el\", \"la\", \"de\", etc.).\n",
    "* Estas palabras suelen eliminarse en tareas de an√°lisis de texto, ya que no aportan mucho significado a nivel de contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09BzO0Byg7IA",
    "outputId": "c4f4365e-c2d3-4fa9-beea-bce746b8263d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14396, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el archivo de datos\n",
    "file_path = r'C:\\Users\\ojito\\Gerencia de Proyectos 2\\Noticias.xlsx'\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Mostrar una vista previa de los datos\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de c√≥digo se encarga de importar bibliotecas, descargar recursos de NLP, cargar el dataset y mostrar su estructura.\n",
    "\n",
    "1. Importaci√≥n de bibliotecas\n",
    "* pandas: Para la manipulaci√≥n y an√°lisis del dataset.\n",
    "* nltk.tokenize.word_tokenize: Para dividir el texto en palabras.\n",
    "* string y re: Para manipulaci√≥n de texto y expresiones regulares.\n",
    "* TfidfVectorizer (Scikit-learn): Para la vectorizaci√≥n de texto usando TF-IDF, una t√©cnica que mide la importancia de cada palabra en un documento.\n",
    "* Word2Vec (Gensim): Para la representaci√≥n de palabras en vectores num√©ricos basados en su contexto en el corpus.\n",
    "\n",
    "2. Descarga de recursos de NLTK\n",
    "* punkt: Para tokenizaci√≥n en NLTK.\n",
    "* stopwords: Contiene palabras vac√≠as en varios idiomas.\n",
    "\n",
    "3. Carga del dataset\n",
    "* Carga el archivo de Excel en un DataFrame de Pandas.\n",
    "* Se asume que el script se ejecuta en Google Colab (por el uso de '/content/' como ruta).\n",
    "\n",
    "4. Visualizaci√≥n de la estructura de los datos\n",
    "* shape devuelve la forma del dataset en (filas, columnas), permitiendo conocer el tama√±o del corpus de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente con 14396 filas y 6 columnas.\n"
     ]
    }
   ],
   "source": [
    "if data.empty:\n",
    "    print(\"Error: El archivo est√° vac√≠o o no se pudo cargar.\")\n",
    "else:\n",
    "    print(f\"Dataset cargado correctamente con {data.shape[0]} filas y {data.shape[1]} columnas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las primeras filas para entender la estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "GOfkKxe4ZE5g",
    "outputId": "e0d6c288-fbed-4b2e-a237-f60b9aaba496"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna1</th>\n",
       "      <th>Enlaces</th>\n",
       "      <th>T√≠tulo</th>\n",
       "      <th>info</th>\n",
       "      <th>contenido</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.eltiempo.com/agresion-contra-un-op...</td>\n",
       "      <td>Operador de gr√∫a qued√≥ inconsciente tras agres...</td>\n",
       "      <td>El conductor de una moto le lanz√≥ el casco y p...</td>\n",
       "      <td>Las autoridades est√°n buscando al conductor de...</td>\n",
       "      <td>colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Usaqu√©n, primera en infracciones por mal parqueo</td>\n",
       "      <td>La localidad ocupa el primer lugar en comparen...</td>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>'Me atracaron y vi un arma que me hel√≥ la sang...</td>\n",
       "      <td>Un ciudadano relata c√≥mo cuatro hombres lo rob...</td>\n",
       "      <td>A las 7 de la noche me hab√≠a quedado de encont...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Escoltas mal estacionados, dolor de cabeza de ...</td>\n",
       "      <td>Las zonas de restaurantes se convierten en par...</td>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Radicado primer proyecto que autorizar√≠a union...</td>\n",
       "      <td>El representante de 'la U', Miguel G√≥mez, dijo...</td>\n",
       "      <td>‚ÄúEstamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Columna1                                            Enlaces  \\\n",
       "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
       "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "\n",
       "                                              T√≠tulo  \\\n",
       "0  Operador de gr√∫a qued√≥ inconsciente tras agres...   \n",
       "1   Usaqu√©n, primera en infracciones por mal parqueo   \n",
       "2  'Me atracaron y vi un arma que me hel√≥ la sang...   \n",
       "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
       "4  Radicado primer proyecto que autorizar√≠a union...   \n",
       "\n",
       "                                                info  \\\n",
       "0  El conductor de una moto le lanz√≥ el casco y p...   \n",
       "1  La localidad ocupa el primer lugar en comparen...   \n",
       "2  Un ciudadano relata c√≥mo cuatro hombres lo rob...   \n",
       "3  Las zonas de restaurantes se convierten en par...   \n",
       "4  El representante de 'la U', Miguel G√≥mez, dijo...   \n",
       "\n",
       "                                           contenido  Etiqueta  \n",
       "0  Las autoridades est√°n buscando al conductor de...  colombia  \n",
       "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
       "2  A las 7 de la noche me hab√≠a quedado de encont...   archivo  \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
       "4  ‚ÄúEstamos proponiendo la figura de un contrato ...   archivo  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploraci√≥n del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Cantidad de noticias por etiqueta:\n",
      "Etiqueta\n",
      "archivo                 9187\n",
      "colombia                 934\n",
      "deportes                 727\n",
      "opinion                  532\n",
      "mundo                    446\n",
      "cultura                  430\n",
      "economia                 367\n",
      "justicia                 343\n",
      "bogota                   311\n",
      "vida                     268\n",
      "politica                 252\n",
      "tecnosfera               214\n",
      "salud                    106\n",
      "historias-el-tiempo       57\n",
      "mundial                   47\n",
      "contenido-comercial       34\n",
      "elecciones                33\n",
      "unidad-investigativa      27\n",
      "podcast                   20\n",
      "foro-w                    18\n",
      "bocas                     15\n",
      "carrusel                   8\n",
      "datos                      7\n",
      "lecturas-dominicales       6\n",
      "mas-contenido              4\n",
      "especiales                 3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä Longitud promedio de los art√≠culos: 531.6849614208764\n",
      "\n",
      "üîù Palabras m√°s frecuentes en el corpus:\n",
      "[('de', 496879), ('la', 253580), ('en', 212954), ('que', 190982), ('el', 181023), ('y', 161176), ('a', 135845), ('del', 112492), ('los', 92772), ('con', 85706)]\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de noticias por categor√≠a\n",
    "print(\"\\nüìå Cantidad de noticias por etiqueta:\")\n",
    "print(data[\"Etiqueta\"].value_counts())\n",
    "\n",
    "# Longitud promedio de los art√≠culos (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "data[\"longitud\"] = data[\"contenido\"].dropna().apply(lambda x: len(str(x).split()))\n",
    "print(\"\\nüìä Longitud promedio de los art√≠culos:\", data[\"longitud\"].mean())\n",
    "\n",
    "# Palabras m√°s frecuentes en el corpus (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "from collections import Counter\n",
    "\n",
    "# Concatenar todos los textos en un solo string\n",
    "texto_completo = \" \".join(data[\"contenido\"].dropna())\n",
    "palabras = texto_completo.split()\n",
    "conteo_palabras = Counter(palabras)\n",
    "\n",
    "# Mostrar las 10 palabras m√°s comunes\n",
    "print(\"\\nüîù Palabras m√°s frecuentes en el corpus:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cantidad de noticias por categor√≠a (Etiqueta)\n",
    "* Se cuenta cu√°ntas noticias hay en cada categor√≠a y se imprime el resultado.\n",
    "  \n",
    "2. Longitud promedio de los art√≠culos (contenido)\n",
    "* Se calcula la cantidad promedio de palabras en los art√≠culos.\n",
    "* Se usa .dropna() para evitar errores con valores nulos.\n",
    "* Se utiliza split() para contar el n√∫mero de palabras en cada art√≠culo.\n",
    "  \n",
    "3. Palabras m√°s frecuentes en el corpus (contenido)\n",
    "* se concatenan todos los art√≠culos en un solo texto.\n",
    "* Se separan las palabras usando split().\n",
    "* Se utiliza Counter de la biblioteca collections para contar la frecuencia de cada palabra.\n",
    "* Se imprimen las 10 palabras m√°s comunes en el corpus de noticias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Palabras M√°s Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîù Palabras m√°s frecuentes en el corpus:\n",
      "[('de', 496879), ('la', 253580), ('en', 212954), ('que', 190982), ('el', 181023), ('y', 161176), ('a', 135845), ('del', 112492), ('los', 92772), ('con', 85706)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Obtener todas las palabras en una sola lista (usando 'contenido' en lugar de 'tokens_sin_stopwords')\n",
    "todas_las_palabras = [word for text in data[\"contenido\"].dropna() for word in text.split()]\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "conteo_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Mostrar las 10 palabras m√°s comunes\n",
    "print(\"\\nüîù Palabras m√°s frecuentes en el corpus:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de c√≥digo cuenta las palabras m√°s frecuentes en el corpus de noticias utilizando la columna contenido. Aqu√≠ est√° el desglose de lo que hace:\n",
    "\n",
    "1. Obtener todas las palabras en una sola lista\n",
    "* Se itera sobre la columna contenido, asegurando que no haya valores nulos con .dropna().\n",
    "* Cada art√≠culo de noticias se divide en palabras usando .split(), generando una lista de todas las palabras.\n",
    "\n",
    "2. Contar la frecuencia de cada palabra\n",
    "* Se utiliza la clase Counter de la biblioteca collections para contar cu√°ntas veces aparece cada palabra en el conjunto de datos.\n",
    "\n",
    "3. Mostrar las 10 palabras m√°s comunes\n",
    "* Se usa most_common(10) para obtener e imprimir las 10 palabras con mayor frecuencia en el corpus."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
