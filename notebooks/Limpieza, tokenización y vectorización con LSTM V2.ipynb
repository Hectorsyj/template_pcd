{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xHJdORHf1N6"
   },
   "source": [
    "# Recolección y Exploración del Corpus de Noticias\n",
    "\n",
    "## Introducción\n",
    "El análisis de texto es una técnica fundamental en el procesamiento del lenguaje natural (NLP), utilizada para extraer información significativa de grandes volúmenes de datos textuales. En este caso, trabajaremos con un conjunto de noticias provenientes de un archivo de datos en Excel, con el objetivo de realizar una carga, exploración, filtrado y análisis del corpus de noticias. Además, se implementarán técnicas de preprocesamiento como limpieza del texto, tokenización y padding, permitiendo mejorar la calidad de los datos procesados para su uso en modelos de aprendizaje profundo.\n",
    "\n",
    "### Este cuaderno Jupyter se enfocará en:\n",
    "\n",
    "1. Carga y visualización del corpus \n",
    "* Importación del archivo de noticias en un DataFrame de Pandas.\n",
    "* Inspección de la estructura y calidad de los datos.\n",
    "* Identificación de valores nulos o inconsistencias en las noticias.\n",
    "\n",
    "2. Exploración del corpus\n",
    "* Análisis estadístico del conjunto de datos.\n",
    "* Conteo de la cantidad de noticias por categoría.\n",
    "* Cálculo de la longitud promedio de los artículos en términos de palabras.\n",
    "\n",
    "3. Palabras más frecuentes\n",
    "* Extracción de todas las palabras del corpus de noticias.\n",
    "* Conteo de la frecuencia de cada palabra en el conjunto de datos.\n",
    "* Identificación de las palabras más utilizadas en los artículos.\n",
    "\n",
    "4. Procesamiento del texto\n",
    "* Limpieza del texto: Conversión a minúsculas, eliminación de caracteres especiales y números.\n",
    "* Tokenización: Conversión del texto en secuencias numéricas.\n",
    "* Padding: Normalización de las secuencias a una longitud fija para modelos de aprendizaje profundo.\n",
    "\n",
    "5. Filtrado de Etiquetas Más Relevantes\n",
    "* Identificación de las etiquetas más comunes en el corpus.\n",
    "* Selección de las 4 etiquetas más frecuentes para reducir el ruido en el análisis.\n",
    "* Filtrado del conjunto de datos para incluir solo las categorías más representativas.\n",
    "\n",
    "## Importancia del Corpus de Noticias\n",
    "El análisis de este corpus nos permitirá identificar tendencias, patrones lingüísticos y relaciones entre palabras, lo cual es útil para aplicaciones como:\n",
    "\n",
    "* Análisis de sentimientos: Determinar la polaridad de las noticias.\n",
    "* Clasificación de noticias: Organizar artículos en categorías relevantes.\n",
    "* Extracción de palabras clave: Identificar términos importantes dentro del corpus.\n",
    "* Modelado de temas: Descubrir patrones y agrupaciones temáticas en los datos.\n",
    "* Filtrado de etiquetas relevantes: Reducir el ruido en la clasificación de noticias y enfocarse en las categorías de mayor interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga y Visualización del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLXwaA3PfYCm",
    "outputId": "97fbaa7d-16a7-4d54-b719-6ace091f6e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código descarga dos recursos clave de la biblioteca NLTK (Natural Language Toolkit), que es una de las herramientas más utilizadas para el procesamiento del lenguaje natural (NLP):\n",
    "\n",
    "1. nltk.download('punkt'):\n",
    "* Descarga el paquete Punkt, que contiene un modelo de tokenización preentrenado.\n",
    "* Se utiliza para dividir un texto en oraciones o palabras, facilitando el análisis del corpus.\n",
    "\n",
    "2. nltk.download('stopwords'):\n",
    "* Descarga una lista de stopwords (palabras vacías o de poco significado, como \"el\", \"la\", \"de\", etc.).\n",
    "* Estas palabras suelen eliminarse en tareas de análisis de texto, ya que no aportan mucho significado a nivel de contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09BzO0Byg7IA",
    "outputId": "c4f4365e-c2d3-4fa9-beea-bce746b8263d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14396, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el archivo de datos\n",
    "file_path = r'C:\\Users\\ojito\\Gerencia de Proyectos 2\\Noticias.xlsx'\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Mostrar una vista previa de los datos\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código se encarga de importar bibliotecas, descargar recursos de NLP, cargar el dataset y mostrar su estructura.\n",
    "\n",
    "1. Importación de bibliotecas\n",
    "* pandas: Para la manipulación y análisis del dataset.\n",
    "* nltk.tokenize.word_tokenize: Para dividir el texto en palabras.\n",
    "* string y re: Para manipulación de texto y expresiones regulares.\n",
    "* TfidfVectorizer (Scikit-learn): Para la vectorización de texto usando TF-IDF, una técnica que mide la importancia de cada palabra en un documento.\n",
    "* Word2Vec (Gensim): Para la representación de palabras en vectores numéricos basados en su contexto en el corpus.\n",
    "\n",
    "2. Descarga de recursos de NLTK\n",
    "* punkt: Para tokenización en NLTK.\n",
    "* stopwords: Contiene palabras vacías en varios idiomas.\n",
    "\n",
    "3. Carga del dataset\n",
    "* Carga el archivo de Excel en un DataFrame de Pandas.\n",
    "* Se asume que el script se ejecuta en Google Colab (por el uso de '/content/' como ruta).\n",
    "\n",
    "4. Visualización de la estructura de los datos\n",
    "* shape devuelve la forma del dataset en (filas, columnas), permitiendo conocer el tamaño del corpus de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente con 14396 filas y 6 columnas.\n"
     ]
    }
   ],
   "source": [
    "if data.empty:\n",
    "    print(\"Error: El archivo está vacío o no se pudo cargar.\")\n",
    "else:\n",
    "    print(f\"Dataset cargado correctamente con {data.shape[0]} filas y {data.shape[1]} columnas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las primeras filas para entender la estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "GOfkKxe4ZE5g",
    "outputId": "e0d6c288-fbed-4b2e-a237-f60b9aaba496"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna1</th>\n",
       "      <th>Enlaces</th>\n",
       "      <th>Título</th>\n",
       "      <th>info</th>\n",
       "      <th>contenido</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.eltiempo.com/agresion-contra-un-op...</td>\n",
       "      <td>Operador de grúa quedó inconsciente tras agres...</td>\n",
       "      <td>El conductor de una moto le lanzó el casco y p...</td>\n",
       "      <td>Las autoridades están buscando al conductor de...</td>\n",
       "      <td>colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Usaquén, primera en infracciones por mal parqueo</td>\n",
       "      <td>La localidad ocupa el primer lugar en comparen...</td>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>'Me atracaron y vi un arma que me heló la sang...</td>\n",
       "      <td>Un ciudadano relata cómo cuatro hombres lo rob...</td>\n",
       "      <td>A las 7 de la noche me había quedado de encont...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Escoltas mal estacionados, dolor de cabeza de ...</td>\n",
       "      <td>Las zonas de restaurantes se convierten en par...</td>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Radicado primer proyecto que autorizaría union...</td>\n",
       "      <td>El representante de 'la U', Miguel Gómez, dijo...</td>\n",
       "      <td>“Estamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Columna1                                            Enlaces  \\\n",
       "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
       "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "\n",
       "                                              Título  \\\n",
       "0  Operador de grúa quedó inconsciente tras agres...   \n",
       "1   Usaquén, primera en infracciones por mal parqueo   \n",
       "2  'Me atracaron y vi un arma que me heló la sang...   \n",
       "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
       "4  Radicado primer proyecto que autorizaría union...   \n",
       "\n",
       "                                                info  \\\n",
       "0  El conductor de una moto le lanzó el casco y p...   \n",
       "1  La localidad ocupa el primer lugar en comparen...   \n",
       "2  Un ciudadano relata cómo cuatro hombres lo rob...   \n",
       "3  Las zonas de restaurantes se convierten en par...   \n",
       "4  El representante de 'la U', Miguel Gómez, dijo...   \n",
       "\n",
       "                                           contenido  Etiqueta  \n",
       "0  Las autoridades están buscando al conductor de...  colombia  \n",
       "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
       "2  A las 7 de la noche me había quedado de encont...   archivo  \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
       "4  “Estamos proponiendo la figura de un contrato ...   archivo  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploración del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Cantidad de noticias por etiqueta:\n",
      "Etiqueta\n",
      "archivo                 9187\n",
      "colombia                 934\n",
      "deportes                 727\n",
      "opinion                  532\n",
      "mundo                    446\n",
      "cultura                  430\n",
      "economia                 367\n",
      "justicia                 343\n",
      "bogota                   311\n",
      "vida                     268\n",
      "politica                 252\n",
      "tecnosfera               214\n",
      "salud                    106\n",
      "historias-el-tiempo       57\n",
      "mundial                   47\n",
      "contenido-comercial       34\n",
      "elecciones                33\n",
      "unidad-investigativa      27\n",
      "podcast                   20\n",
      "foro-w                    18\n",
      "bocas                     15\n",
      "carrusel                   8\n",
      "datos                      7\n",
      "lecturas-dominicales       6\n",
      "mas-contenido              4\n",
      "especiales                 3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📊 Longitud promedio de los artículos: 531.6849614208764\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de noticias por categoría\n",
    "print(\"\\n📌 Cantidad de noticias por etiqueta:\")\n",
    "print(data[\"Etiqueta\"].value_counts())\n",
    "\n",
    "# Longitud promedio de los artículos (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "data[\"longitud\"] = data[\"contenido\"].dropna().apply(lambda x: len(str(x).split()))\n",
    "print(\"\\n📊 Longitud promedio de los artículos:\", data[\"longitud\"].mean())\n",
    "\n",
    "# Palabras más frecuentes en el corpus (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "from collections import Counter\n",
    "\n",
    "# Concatenar todos los textos en un solo string\n",
    "texto_completo = \" \".join(data[\"contenido\"].dropna())\n",
    "palabras = texto_completo.split()\n",
    "conteo_palabras = Counter(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cantidad de noticias por categoría (Etiqueta)\n",
    "* Se cuenta cuántas noticias hay en cada categoría y se imprime el resultado.\n",
    "  \n",
    "2. Longitud promedio de los artículos (contenido)\n",
    "* Se calcula la cantidad promedio de palabras en los artículos.\n",
    "* Se usa .dropna() para evitar errores con valores nulos.\n",
    "* Se utiliza split() para contar el número de palabras en cada artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Palabras Más Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔝 Palabras más frecuentes en el corpus:\n",
      "[('de', 496879), ('la', 253580), ('en', 212954), ('que', 190982), ('el', 181023), ('y', 161176), ('a', 135845), ('del', 112492), ('los', 92772), ('con', 85706)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Obtener todas las palabras en una sola lista (usando 'contenido' en lugar de 'tokens_sin_stopwords')\n",
    "todas_las_palabras = [word for text in data[\"contenido\"].dropna() for word in text.split()]\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "conteo_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Mostrar las 10 palabras más comunes\n",
    "print(\"\\n🔝 Palabras más frecuentes en el corpus:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código cuenta las palabras más frecuentes en el corpus de noticias utilizando la columna contenido. Aquí está el desglose de lo que hace:\n",
    "\n",
    "1. Obtener todas las palabras en una sola lista\n",
    "* Se itera sobre la columna contenido, asegurando que no haya valores nulos con .dropna().\n",
    "* Cada artículo de noticias se divide en palabras usando .split(), generando una lista de todas las palabras.\n",
    "\n",
    "2. Contar la frecuencia de cada palabra\n",
    "* Se utiliza la clase Counter de la biblioteca collections para contar cuántas veces aparece cada palabra en el conjunto de datos.\n",
    "\n",
    "3. Mostrar las 10 palabras más comunes\n",
    "* Se usa most_common(10) para obtener e imprimir las 10 palabras con mayor frecuencia en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza, tokenización y vectorización con LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizer: Se utiliza para convertir el texto en secuencias numéricas.\n",
    "* pad_sequences: Ayuda a normalizar la longitud de las secuencias, agregando padding si es necesario.\n",
    "* numpy: Para manipular datos numéricos.\n",
    "* re: Librería de expresiones regulares para limpiar texto.\n",
    "* pickle: Para guardar el tokenizer y reutilizarlo después."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Definir parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "MAX_NB_WORDS = 5000  # Número máximo de palabras en el vocabulario\n",
    "MAX_SEQUENCE_LENGTH = 200  # Longitud máxima de las secuencias\n",
    "EMBEDDING_DIM = 100  # Dimensión de los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos valores se utilizan para configurar el procesamiento del texto:\n",
    "\n",
    "* MAX_NB_WORDS = 5000: Solo se considerarán las 5000 palabras más frecuentes en el vocabulario.\n",
    "* MAX_SEQUENCE_LENGTH = 200: Todas las secuencias serán truncadas o rellenadas hasta 200 palabras.\n",
    "* EMBEDDING_DIM = 100: Cada palabra en el modelo LSTM se representará con un vector de 100 dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Función para limpiar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Lista de stopwords en español\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Función para limpiar el texto y eliminar stopwords\n",
    "def limpiar_texto(texto):\n",
    "    if isinstance(texto, float):  # Si es NaN, lo convertimos a una cadena vacía\n",
    "        return \"\"\n",
    "    texto = str(texto).lower()  # Convertir a minúsculas\n",
    "    texto = re.sub(r'[^a-záéíóúüñ ]', '', texto)  # Eliminar caracteres especiales y números\n",
    "    palabras = texto.split()  # Tokenizar el texto dividiéndolo por espacios\n",
    "    palabras_filtradas = [word for word in palabras if word not in stop_words]  # Eliminar stopwords\n",
    "    return \" \".join(palabras_filtradas)  # Volver a unir las palabras en una cadena\n",
    "\n",
    "# Aplicar la limpieza al conjunto de datos\n",
    "data[\"contenido_limpio\"] = data[\"contenido\"].astype(str).apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convierte texto a minúsculas\n",
    "* Elimina caracteres especiales y números\n",
    "* Elimina stopwords en español\n",
    "* Reconstruye el texto limpio\n",
    "* Aplica la limpieza a todo el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔝 Palabras más frecuentes en el corpus después de eliminación de stopwords:\n",
      "[('pm', 72074), ('colombia', 37084), ('valle', 28618), ('manizales', 22344), ('coronavirus', 18701), ('mil', 18185), ('mercados', 17190), ('barranquilla', 15978), ('cerca', 15634), ('cauca', 14515)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Obtener todas las palabras en una sola lista después de la limpieza\n",
    "todas_las_palabras = [word for text in data[\"contenido_limpio\"].dropna() for word in text.split()]\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "conteo_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Mostrar las 10 palabras más comunes después de eliminar stopwords\n",
    "print(\"\\n🔝 Palabras más frecuentes en el corpus después de eliminación de stopwords:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Counter es una herramienta de la librería estándar collections de Python: Se usa para contar la frecuencia de elementos dentro de una lista.\n",
    "* Extrae todas las palabras de la columna \"contenido_limpio\" del DataFrame data.\n",
    "* .dropna(): Se usa para eliminar valores nulos (NaN) y evitar errores.\n",
    "* Tokenización simple (split()): Divide cada texto en palabras individuales.\n",
    "* Comprensión de listas (list comprehension): Recorre todas las filas y extrae cada palabra.\n",
    "* Usa Counter para contar cuántas veces aparece cada palabra en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Aplicar limpieza a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar limpieza asegurando que todos los valores sean cadenas de texto\n",
    "data[\"contenido_limpio\"] = data[\"contenido\"].astype(str).apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplica la función limpiar_texto() a la columna contenido del dataset.\n",
    "* Asegura que todos los valores son cadenas (astype(str)) antes de aplicar la limpieza.\n",
    "* Guarda el resultado en la nueva columna contenido_limpio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Función para tokenizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de tokenización\n",
    "def tokenizar_texto(textos):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(textos)\n",
    "    sequences = tokenizer.texts_to_sequences(textos)\n",
    "    return tokenizer, sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función:\n",
    "\n",
    "* Crea un tokenizador que usa MAX_NB_WORDS=5000 palabras más frecuentes.\n",
    "* Usa el token <OOV> (Out of Vocabulary) para palabras desconocidas.\n",
    "* Convierte el texto en secuencias de números, donde cada palabra se convierte en su índice en el vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Aplicar tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar tokenización\n",
    "tokenizer, sequences = tokenizar_texto(data[\"contenido_limpio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokeniza el texto limpio, convirtiéndolo en secuencias de números.\n",
    "* Cada palabra tiene un número asociado según el tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Padding para normalizar las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding para hacer las secuencias del mismo tamaño\n",
    "def aplicar_padding(sequences, max_length):\n",
    "    return pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función:\n",
    "\n",
    "* Asegura que todas las secuencias tengan el mismo largo máximo (MAX_SEQUENCE_LENGTH = 200).\n",
    "* Usa padding='post', lo que significa que si la secuencia es más corta, se agregan ceros al final.\n",
    "* Usa truncating='post', lo que significa que si la secuencia es más larga, se corta al final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Aplicar padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aplicar_padding(sequences, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convierte las secuencias tokenizadas en matrices listas para LSTM.\n",
    "* X ahora es una matriz (número de ejemplos, 200).\n",
    "* Ejemplo de salida:\n",
    "\n",
    "Antes del padding:\n",
    "[5, 87, 302, 15, 928]\n",
    "\n",
    "Después del padding:\n",
    "[5, 87, 302, 15, 928, 0, 0, 0, 0, ..., 0]  (hasta 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Imprimir información sobre los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Total de palabras en el vocabulario: 123089\n",
      "🔠 Forma de una secuencia de entrada: (14396, 200)\n"
     ]
    }
   ],
   "source": [
    "# Imprimir información sobre los datos tokenizados\n",
    "print(f\"🔢 Total de palabras en el vocabulario: {len(tokenizer.word_index)}\")\n",
    "print(f\"🔠 Forma de una secuencia de entrada: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cantidad de palabras en el vocabulario (tokenizer.word_index).\n",
    "* Dimensiones de las secuencias (X.shape)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Guardar el tokenizer para uso futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar tokenizer para uso futuro\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Guarda el tokenizer en un archivo pickle (.pkl) para reutilizarlo sin volver a tokenizar el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Muestra las primeras 5 secuencias tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "primeras 5 secuencias procesadas:\n",
      "[[ 180 1753 1680 2827 2178 4278  169  889    1 3790    1  201    1 1127\n",
      "   297 1014  114 4666  908  544 1806  108    1  201    1  807    1   10\n",
      "  1074 1402 3870 3628    1  527    1  674    1    1 2990  609  234 1680\n",
      "  2827    1 4278 3790  133 2179  601 1010 1369    1  934    1  608    1\n",
      "  4397    1 1127 1014   10  297    1    1    1    1  149    1 1080 2797\n",
      "   938 3790 2138  113 2959    1 2827    1  149  890  420    1    1  180\n",
      "  1457  133    1 2827    1 1768  489    1 1705 1229    1 4666 1954    1\n",
      "    24  453    1  340  404  640  587  170   90  485 3492   99  103  201\n",
      "     1    1 1464 1052   99    1  242   76    4   11    2   15   10    7\n",
      "     8   16    4   14   93   83    7    8   41  110   77    6    3    2\n",
      "    12   58   64   65   38  160  135  190  244  277  312  125   55   68\n",
      "     2  132  143  141   67  146   55   68    9    2  111  145   91   95\n",
      "   140  112  188    2  133  241  192  187  325  207    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   1 4628    1    1    1    1 2804 4366    1 3432 3408 2614  233  223\n",
      "  3050    1  301    1 1449 1486  279   97  151  685   90    1    1 4338\n",
      "     1  445 2166    1  165 3198  353    1  233  259    1    1  296  418\n",
      "   835 1107 1486  279 2082    1    1  993    1    1 1505    1 3050  353\n",
      "   558  445    1  838    1 1681    1    1  107  423 2648 3514 3408 2570\n",
      "    92  259  233    1    1  353  197  445  871 2818  138    1    1 1350\n",
      "     1 1027  658 3051 2632    1  151  622  459    1 2818 2066 1927 3075\n",
      "     7   82    1 3791  799    1 1807 1155    1  668  748    1  232 1074\n",
      "  2728   71 1928  233 2026 3572  231    1  587    1 1105 3106  231  755\n",
      "   587 1123 3759 2095 1195  353 2943  577    1    1 3572 2520    1  327\n",
      "   280   24  227  771 1088 1068    1    1    1  398  790 3677    1    1\n",
      "  3871 1195    1  616    1 1249 3051  558  328 2246    1  165  333  180\n",
      "  3241  122  176  165  587  576  492    1 3461 1796    1 2729    1 2046\n",
      "     1    1    1 3662  400 2729  276    1 2944  800 2765 1648  362 1442\n",
      "  1468    1    1  283]\n",
      " [ 302 3255  898    1  171   89    1   87 2920    1  319  866  144 1670\n",
      "     1 1241    1  319    1    1 3272 4367    1  843    1    1    1  144\n",
      "     1 2038 3020  275  302 2932 3353  192 3091 2521 1764    1    1  571\n",
      "     1    1    1  223 3010  164   41    1 4751 1132  424  324    1 3272\n",
      "    37 1261 2740  183 2979    1 1505   78    1  346 3629  744  386    1\n",
      "     1  398 2499   44  164    1    1    1 1236    1    1  107    1 3076\n",
      "     1   60   37    1    1    1    1  290 3010 2062    1 1754 1346    1\n",
      "  1435    1    1 3629    1 4461 4558    1   18 1538 1435   88 3433  243\n",
      "   228   89    1    1    1    1    1 1797    1  784  568 2740    1 4197\n",
      "  4629   71    7   82 3629   79 2598 2294    1   37 4920    1   37 1065\n",
      "  2615    1   37 1065 2543  608   37 1065    1    1  470    1  197 1480\n",
      "   103 3462  129    1    1   37  571    1    1  197 1195    1 3272  853\n",
      "  2615  716    1    1    1    1  144 1180 2441    1 1670  136  103 4485\n",
      "   194  501   44    1   71    1 2730  950  150   56 2960    1  810  532\n",
      "   106 2200    1    1]\n",
      " [   1   65  608 2943    1 4253    1  506    1    1  862 2679 3573    1\n",
      "   762    1  645  192  233 1910 1074 1726    1 4873  233 1726    1    1\n",
      "   103 2209    1 1972  165 1080  327    1 4792 1249   98  740  549  144\n",
      "  2598    1  122    1  815 4023    1 4520  233  223 4873   10    1  732\n",
      "  1967  732    3   37   41 4023    1    1    1    1  353  269    1    1\n",
      "    99  311   56    1  887 2359    1    1  151  622    1  576  492    1\n",
      "  3021    1    1  150 1682   37 4874    1 2062  443   41 2114    1  632\n",
      "  1722  367  136  800  215    1  506    1  353 4594  708  136 1632    1\n",
      "     1    1  409 2046  100    1  435  106    1 4630    1    1  507  116\n",
      "  1363    1  506    1 1609    1    1  662  748    1  117   10 1769    1\n",
      "  2500    1    1   71  909    1  269  870    1  353 1702    1    1 1129\n",
      "   836   72    1 1544    1  353  976  587  576  492    1 4399  441  487\n",
      "  1567 1195  506    1 2680    1 2115    1  430   63  199    4   11    2\n",
      "    15   10    7    8   16    4   14   93   83    7    8   41  110   77\n",
      "     6    3    2   12]\n",
      " [   1  928  769  996  761 1443    1   69 2561    1  126 2561    1  928\n",
      "  3872 2252  360    1  513  109  313 4339  216  219    1  152  193  550\n",
      "   358  304 2465    1  266  867  283 2501   78  377  289  780    1  323\n",
      "     1    1 2561   92 2368    1   44   24  425  496 1130 2271    1  733\n",
      "   496 3515  637 1117  496 4072  928  893  513 4667  315  769   72    1\n",
      "  4486   92 2252    1    1  928  761  218  893  928  218    1 1248  299\n",
      "   761  133  295  877  216  474    1    1  688    1 4400  474    1    1\n",
      "   103    1 3077 2252    1    1  160    1  513 1061 1996    1    1    1\n",
      "  2561   92 2368  991 1075    1    1  372  500   37 3409  816  980  206\n",
      "  4714  258 2007  377  266  867 4003 2561    1  167 3130  218 2348 3077\n",
      "  2252 2413    3  360  133  295  648  323 2589    1 2561   92 2368  266\n",
      "   257   37  500  323 2502  816 2681 2561    1  823  514  275    1  286\n",
      "     1 2252    1 1035    1 1489 2027  761 1076    1    1  773  474   81\n",
      "     1  359   37 1544   71    1   60  733  935    1 3732  761    1    1\n",
      "   198  315    1 1208]]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar algunas secuencias tokenizadas\n",
    "print(\"\\nprimeras 5 secuencias procesadas:\")\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cada fila representa un texto convertido en números y normalizado para usarlo en LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Definir la función obtener_etiquetas_relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_etiquetas_relevantes(data, columna_etiqueta=\"Etiqueta\", top_n=4):\n",
    "    etiquetas_comunes = Counter(data[columna_etiqueta]).most_common(top_n)\n",
    "    etiquetas_relevantes = [etiqueta[0] for etiqueta in etiquetas_comunes]\n",
    "    return etiquetas_relevantes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué hace esta función?\n",
    "\n",
    "* Cuenta la frecuencia de cada etiqueta en la columna \"Etiqueta\" utilizando Counter de collections.\n",
    "* Obtiene las top_n etiquetas más comunes con .most_common(top_n).\n",
    "* top_n=4 significa que se seleccionarán las 4 etiquetas más frecuentes.\n",
    "* Extrae solo los nombres de las etiquetas en una lista.\n",
    "* Devuelve la lista de etiquetas más comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Obtener y mostrar las etiquetas más relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Etiquetas más relevantes: ['archivo', 'colombia', 'deportes', 'opinion']\n"
     ]
    }
   ],
   "source": [
    "etiquetas_mas_relevantes = obtener_etiquetas_relevantes(data)\n",
    "print(f\"\\n🎯 Etiquetas más relevantes: {etiquetas_mas_relevantes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se llama a la función obtener_etiquetas_relevantes(data).\n",
    "* Se almacenan las etiquetas más comunes en etiquetas_mas_relevantes.\n",
    "* Se imprimen las etiquetas seleccionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Filtrar el dataset para conservar solo esas etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Nuevo tamaño del dataset después del filtro: (11380, 8)\n"
     ]
    }
   ],
   "source": [
    "data_filtrada = data[data[\"Etiqueta\"].isin(etiquetas_mas_relevantes)]\n",
    "print(f\"\\n📌 Nuevo tamaño del dataset después del filtro: {data_filtrada.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esto indica que, después del filtrado, el dataset ahora tiene 11380 filas y 8 columnas."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
