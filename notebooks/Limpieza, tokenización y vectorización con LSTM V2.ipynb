{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xHJdORHf1N6"
   },
   "source": [
    "# Recolecci√≥n y Exploraci√≥n del Corpus de Noticias\n",
    "\n",
    "## Introducci√≥n\n",
    "El an√°lisis de texto es una t√©cnica fundamental en el procesamiento del lenguaje natural (NLP), utilizada para extraer informaci√≥n significativa de grandes vol√∫menes de datos textuales. En este caso, trabajaremos con un conjunto de noticias provenientes de un archivo de datos en Excel, con el objetivo de realizar una carga, exploraci√≥n, filtrado y an√°lisis del corpus de noticias. Adem√°s, se implementar√°n t√©cnicas de preprocesamiento como limpieza del texto, tokenizaci√≥n y padding, permitiendo mejorar la calidad de los datos procesados para su uso en modelos de aprendizaje profundo.\n",
    "\n",
    "### Este cuaderno Jupyter se enfocar√° en:\n",
    "\n",
    "1. Carga y visualizaci√≥n del corpus \n",
    "* Importaci√≥n del archivo de noticias en un DataFrame de Pandas.\n",
    "* Inspecci√≥n de la estructura y calidad de los datos.\n",
    "* Identificaci√≥n de valores nulos o inconsistencias en las noticias.\n",
    "\n",
    "2. Exploraci√≥n del corpus\n",
    "* An√°lisis estad√≠stico del conjunto de datos.\n",
    "* Conteo de la cantidad de noticias por categor√≠a.\n",
    "* C√°lculo de la longitud promedio de los art√≠culos en t√©rminos de palabras.\n",
    "\n",
    "3. Palabras m√°s frecuentes\n",
    "* Extracci√≥n de todas las palabras del corpus de noticias.\n",
    "* Conteo de la frecuencia de cada palabra en el conjunto de datos.\n",
    "* Identificaci√≥n de las palabras m√°s utilizadas en los art√≠culos.\n",
    "\n",
    "4. Procesamiento del texto\n",
    "* Limpieza del texto: Conversi√≥n a min√∫sculas, eliminaci√≥n de caracteres especiales y n√∫meros.\n",
    "* Tokenizaci√≥n: Conversi√≥n del texto en secuencias num√©ricas.\n",
    "* Padding: Normalizaci√≥n de las secuencias a una longitud fija para modelos de aprendizaje profundo.\n",
    "\n",
    "5. Filtrado de Etiquetas M√°s Relevantes\n",
    "* Identificaci√≥n de las etiquetas m√°s comunes en el corpus.\n",
    "* Selecci√≥n de las 4 etiquetas m√°s frecuentes para reducir el ruido en el an√°lisis.\n",
    "* Filtrado del conjunto de datos para incluir solo las categor√≠as m√°s representativas.\n",
    "\n",
    "## Importancia del Corpus de Noticias\n",
    "El an√°lisis de este corpus nos permitir√° identificar tendencias, patrones ling√º√≠sticos y relaciones entre palabras, lo cual es √∫til para aplicaciones como:\n",
    "\n",
    "* An√°lisis de sentimientos: Determinar la polaridad de las noticias.\n",
    "* Clasificaci√≥n de noticias: Organizar art√≠culos en categor√≠as relevantes.\n",
    "* Extracci√≥n de palabras clave: Identificar t√©rminos importantes dentro del corpus.\n",
    "* Modelado de temas: Descubrir patrones y agrupaciones tem√°ticas en los datos.\n",
    "* Filtrado de etiquetas relevantes: Reducir el ruido en la clasificaci√≥n de noticias y enfocarse en las categor√≠as de mayor inter√©s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga y Visualizaci√≥n del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLXwaA3PfYCm",
    "outputId": "97fbaa7d-16a7-4d54-b719-6ace091f6e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de c√≥digo descarga dos recursos clave de la biblioteca NLTK (Natural Language Toolkit), que es una de las herramientas m√°s utilizadas para el procesamiento del lenguaje natural (NLP):\n",
    "\n",
    "1. nltk.download('punkt'):\n",
    "* Descarga el paquete Punkt, que contiene un modelo de tokenizaci√≥n preentrenado.\n",
    "* Se utiliza para dividir un texto en oraciones o palabras, facilitando el an√°lisis del corpus.\n",
    "\n",
    "2. nltk.download('stopwords'):\n",
    "* Descarga una lista de stopwords (palabras vac√≠as o de poco significado, como \"el\", \"la\", \"de\", etc.).\n",
    "* Estas palabras suelen eliminarse en tareas de an√°lisis de texto, ya que no aportan mucho significado a nivel de contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09BzO0Byg7IA",
    "outputId": "c4f4365e-c2d3-4fa9-beea-bce746b8263d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ojito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14396, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el archivo de datos\n",
    "file_path = r'C:\\Users\\ojito\\Gerencia de Proyectos 2\\Noticias.xlsx'\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Mostrar una vista previa de los datos\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de c√≥digo se encarga de importar bibliotecas, descargar recursos de NLP, cargar el dataset y mostrar su estructura.\n",
    "\n",
    "1. Importaci√≥n de bibliotecas\n",
    "* pandas: Para la manipulaci√≥n y an√°lisis del dataset.\n",
    "* nltk.tokenize.word_tokenize: Para dividir el texto en palabras.\n",
    "* string y re: Para manipulaci√≥n de texto y expresiones regulares.\n",
    "* TfidfVectorizer (Scikit-learn): Para la vectorizaci√≥n de texto usando TF-IDF, una t√©cnica que mide la importancia de cada palabra en un documento.\n",
    "* Word2Vec (Gensim): Para la representaci√≥n de palabras en vectores num√©ricos basados en su contexto en el corpus.\n",
    "\n",
    "2. Descarga de recursos de NLTK\n",
    "* punkt: Para tokenizaci√≥n en NLTK.\n",
    "* stopwords: Contiene palabras vac√≠as en varios idiomas.\n",
    "\n",
    "3. Carga del dataset\n",
    "* Carga el archivo de Excel en un DataFrame de Pandas.\n",
    "* Se asume que el script se ejecuta en Google Colab (por el uso de '/content/' como ruta).\n",
    "\n",
    "4. Visualizaci√≥n de la estructura de los datos\n",
    "* shape devuelve la forma del dataset en (filas, columnas), permitiendo conocer el tama√±o del corpus de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente con 14396 filas y 6 columnas.\n"
     ]
    }
   ],
   "source": [
    "if data.empty:\n",
    "    print(\"Error: El archivo est√° vac√≠o o no se pudo cargar.\")\n",
    "else:\n",
    "    print(f\"Dataset cargado correctamente con {data.shape[0]} filas y {data.shape[1]} columnas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las primeras filas para entender la estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "GOfkKxe4ZE5g",
    "outputId": "e0d6c288-fbed-4b2e-a237-f60b9aaba496"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna1</th>\n",
       "      <th>Enlaces</th>\n",
       "      <th>T√≠tulo</th>\n",
       "      <th>info</th>\n",
       "      <th>contenido</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.eltiempo.com/agresion-contra-un-op...</td>\n",
       "      <td>Operador de gr√∫a qued√≥ inconsciente tras agres...</td>\n",
       "      <td>El conductor de una moto le lanz√≥ el casco y p...</td>\n",
       "      <td>Las autoridades est√°n buscando al conductor de...</td>\n",
       "      <td>colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Usaqu√©n, primera en infracciones por mal parqueo</td>\n",
       "      <td>La localidad ocupa el primer lugar en comparen...</td>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>'Me atracaron y vi un arma que me hel√≥ la sang...</td>\n",
       "      <td>Un ciudadano relata c√≥mo cuatro hombres lo rob...</td>\n",
       "      <td>A las 7 de la noche me hab√≠a quedado de encont...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Escoltas mal estacionados, dolor de cabeza de ...</td>\n",
       "      <td>Las zonas de restaurantes se convierten en par...</td>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Radicado primer proyecto que autorizar√≠a union...</td>\n",
       "      <td>El representante de 'la U', Miguel G√≥mez, dijo...</td>\n",
       "      <td>‚ÄúEstamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Columna1                                            Enlaces  \\\n",
       "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
       "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "\n",
       "                                              T√≠tulo  \\\n",
       "0  Operador de gr√∫a qued√≥ inconsciente tras agres...   \n",
       "1   Usaqu√©n, primera en infracciones por mal parqueo   \n",
       "2  'Me atracaron y vi un arma que me hel√≥ la sang...   \n",
       "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
       "4  Radicado primer proyecto que autorizar√≠a union...   \n",
       "\n",
       "                                                info  \\\n",
       "0  El conductor de una moto le lanz√≥ el casco y p...   \n",
       "1  La localidad ocupa el primer lugar en comparen...   \n",
       "2  Un ciudadano relata c√≥mo cuatro hombres lo rob...   \n",
       "3  Las zonas de restaurantes se convierten en par...   \n",
       "4  El representante de 'la U', Miguel G√≥mez, dijo...   \n",
       "\n",
       "                                           contenido  Etiqueta  \n",
       "0  Las autoridades est√°n buscando al conductor de...  colombia  \n",
       "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
       "2  A las 7 de la noche me hab√≠a quedado de encont...   archivo  \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
       "4  ‚ÄúEstamos proponiendo la figura de un contrato ...   archivo  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploraci√≥n del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Cantidad de noticias por etiqueta:\n",
      "Etiqueta\n",
      "archivo                 9187\n",
      "colombia                 934\n",
      "deportes                 727\n",
      "opinion                  532\n",
      "mundo                    446\n",
      "cultura                  430\n",
      "economia                 367\n",
      "justicia                 343\n",
      "bogota                   311\n",
      "vida                     268\n",
      "politica                 252\n",
      "tecnosfera               214\n",
      "salud                    106\n",
      "historias-el-tiempo       57\n",
      "mundial                   47\n",
      "contenido-comercial       34\n",
      "elecciones                33\n",
      "unidad-investigativa      27\n",
      "podcast                   20\n",
      "foro-w                    18\n",
      "bocas                     15\n",
      "carrusel                   8\n",
      "datos                      7\n",
      "lecturas-dominicales       6\n",
      "mas-contenido              4\n",
      "especiales                 3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä Longitud promedio de los art√≠culos: 531.6849614208764\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de noticias por categor√≠a\n",
    "print(\"\\nüìå Cantidad de noticias por etiqueta:\")\n",
    "print(data[\"Etiqueta\"].value_counts())\n",
    "\n",
    "# Longitud promedio de los art√≠culos (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "data[\"longitud\"] = data[\"contenido\"].dropna().apply(lambda x: len(str(x).split()))\n",
    "print(\"\\nüìä Longitud promedio de los art√≠culos:\", data[\"longitud\"].mean())\n",
    "\n",
    "# Palabras m√°s frecuentes en el corpus (usando 'contenido' en lugar de 'contenido_preprocesado')\n",
    "from collections import Counter\n",
    "\n",
    "# Concatenar todos los textos en un solo string\n",
    "texto_completo = \" \".join(data[\"contenido\"].dropna())\n",
    "palabras = texto_completo.split()\n",
    "conteo_palabras = Counter(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cantidad de noticias por categor√≠a (Etiqueta)\n",
    "* Se cuenta cu√°ntas noticias hay en cada categor√≠a y se imprime el resultado.\n",
    "  \n",
    "2. Longitud promedio de los art√≠culos (contenido)\n",
    "* Se calcula la cantidad promedio de palabras en los art√≠culos.\n",
    "* Se usa .dropna() para evitar errores con valores nulos.\n",
    "* Se utiliza split() para contar el n√∫mero de palabras en cada art√≠culo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Palabras M√°s Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîù Palabras m√°s frecuentes en el corpus:\n",
      "[('de', 496879), ('la', 253580), ('en', 212954), ('que', 190982), ('el', 181023), ('y', 161176), ('a', 135845), ('del', 112492), ('los', 92772), ('con', 85706)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Obtener todas las palabras en una sola lista (usando 'contenido' en lugar de 'tokens_sin_stopwords')\n",
    "todas_las_palabras = [word for text in data[\"contenido\"].dropna() for word in text.split()]\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "conteo_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Mostrar las 10 palabras m√°s comunes\n",
    "print(\"\\nüîù Palabras m√°s frecuentes en el corpus:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de c√≥digo cuenta las palabras m√°s frecuentes en el corpus de noticias utilizando la columna contenido. Aqu√≠ est√° el desglose de lo que hace:\n",
    "\n",
    "1. Obtener todas las palabras en una sola lista\n",
    "* Se itera sobre la columna contenido, asegurando que no haya valores nulos con .dropna().\n",
    "* Cada art√≠culo de noticias se divide en palabras usando .split(), generando una lista de todas las palabras.\n",
    "\n",
    "2. Contar la frecuencia de cada palabra\n",
    "* Se utiliza la clase Counter de la biblioteca collections para contar cu√°ntas veces aparece cada palabra en el conjunto de datos.\n",
    "\n",
    "3. Mostrar las 10 palabras m√°s comunes\n",
    "* Se usa most_common(10) para obtener e imprimir las 10 palabras con mayor frecuencia en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza, tokenizaci√≥n y vectorizaci√≥n con LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importaci√≥n de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizer: Se utiliza para convertir el texto en secuencias num√©ricas.\n",
    "* pad_sequences: Ayuda a normalizar la longitud de las secuencias, agregando padding si es necesario.\n",
    "* numpy: Para manipular datos num√©ricos.\n",
    "* re: Librer√≠a de expresiones regulares para limpiar texto.\n",
    "* pickle: Para guardar el tokenizer y reutilizarlo despu√©s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Definir par√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros\n",
    "MAX_NB_WORDS = 5000  # N√∫mero m√°ximo de palabras en el vocabulario\n",
    "MAX_SEQUENCE_LENGTH = 200  # Longitud m√°xima de las secuencias\n",
    "EMBEDDING_DIM = 100  # Dimensi√≥n de los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos valores se utilizan para configurar el procesamiento del texto:\n",
    "\n",
    "* MAX_NB_WORDS = 5000: Solo se considerar√°n las 5000 palabras m√°s frecuentes en el vocabulario.\n",
    "* MAX_SEQUENCE_LENGTH = 200: Todas las secuencias ser√°n truncadas o rellenadas hasta 200 palabras.\n",
    "* EMBEDDING_DIM = 100: Cada palabra en el modelo LSTM se representar√° con un vector de 100 dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Funci√≥n para limpiar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Lista de stopwords en espa√±ol\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Funci√≥n para limpiar el texto y eliminar stopwords\n",
    "def limpiar_texto(texto):\n",
    "    if isinstance(texto, float):  # Si es NaN, lo convertimos a una cadena vac√≠a\n",
    "        return \"\"\n",
    "    texto = str(texto).lower()  # Convertir a min√∫sculas\n",
    "    texto = re.sub(r'[^a-z√°√©√≠√≥√∫√º√± ]', '', texto)  # Eliminar caracteres especiales y n√∫meros\n",
    "    palabras = texto.split()  # Tokenizar el texto dividi√©ndolo por espacios\n",
    "    palabras_filtradas = [word for word in palabras if word not in stop_words]  # Eliminar stopwords\n",
    "    return \" \".join(palabras_filtradas)  # Volver a unir las palabras en una cadena\n",
    "\n",
    "# Aplicar la limpieza al conjunto de datos\n",
    "data[\"contenido_limpio\"] = data[\"contenido\"].astype(str).apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convierte texto a min√∫sculas\n",
    "* Elimina caracteres especiales y n√∫meros\n",
    "* Elimina stopwords en espa√±ol\n",
    "* Reconstruye el texto limpio\n",
    "* Aplica la limpieza a todo el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîù Palabras m√°s frecuentes en el corpus despu√©s de eliminaci√≥n de stopwords:\n",
      "[('pm', 72074), ('colombia', 37084), ('valle', 28618), ('manizales', 22344), ('coronavirus', 18701), ('mil', 18185), ('mercados', 17190), ('barranquilla', 15978), ('cerca', 15634), ('cauca', 14515)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Obtener todas las palabras en una sola lista despu√©s de la limpieza\n",
    "todas_las_palabras = [word for text in data[\"contenido_limpio\"].dropna() for word in text.split()]\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "conteo_palabras = Counter(todas_las_palabras)\n",
    "\n",
    "# Mostrar las 10 palabras m√°s comunes despu√©s de eliminar stopwords\n",
    "print(\"\\nüîù Palabras m√°s frecuentes en el corpus despu√©s de eliminaci√≥n de stopwords:\")\n",
    "print(conteo_palabras.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Counter es una herramienta de la librer√≠a est√°ndar collections de Python: Se usa para contar la frecuencia de elementos dentro de una lista.\n",
    "* Extrae todas las palabras de la columna \"contenido_limpio\" del DataFrame data.\n",
    "* .dropna(): Se usa para eliminar valores nulos (NaN) y evitar errores.\n",
    "* Tokenizaci√≥n simple (split()): Divide cada texto en palabras individuales.\n",
    "* Comprensi√≥n de listas (list comprehension): Recorre todas las filas y extrae cada palabra.\n",
    "* Usa Counter para contar cu√°ntas veces aparece cada palabra en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Aplicar limpieza a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar limpieza asegurando que todos los valores sean cadenas de texto\n",
    "data[\"contenido_limpio\"] = data[\"contenido\"].astype(str).apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplica la funci√≥n limpiar_texto() a la columna contenido del dataset.\n",
    "* Asegura que todos los valores son cadenas (astype(str)) antes de aplicar la limpieza.\n",
    "* Guarda el resultado en la nueva columna contenido_limpio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Funci√≥n para tokenizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de tokenizaci√≥n\n",
    "def tokenizar_texto(textos):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(textos)\n",
    "    sequences = tokenizer.texts_to_sequences(textos)\n",
    "    return tokenizer, sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funci√≥n:\n",
    "\n",
    "* Crea un tokenizador que usa MAX_NB_WORDS=5000 palabras m√°s frecuentes.\n",
    "* Usa el token <OOV> (Out of Vocabulary) para palabras desconocidas.\n",
    "* Convierte el texto en secuencias de n√∫meros, donde cada palabra se convierte en su √≠ndice en el vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Aplicar tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar tokenizaci√≥n\n",
    "tokenizer, sequences = tokenizar_texto(data[\"contenido_limpio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokeniza el texto limpio, convirti√©ndolo en secuencias de n√∫meros.\n",
    "* Cada palabra tiene un n√∫mero asociado seg√∫n el tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Padding para normalizar las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding para hacer las secuencias del mismo tama√±o\n",
    "def aplicar_padding(sequences, max_length):\n",
    "    return pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funci√≥n:\n",
    "\n",
    "* Asegura que todas las secuencias tengan el mismo largo m√°ximo (MAX_SEQUENCE_LENGTH = 200).\n",
    "* Usa padding='post', lo que significa que si la secuencia es m√°s corta, se agregan ceros al final.\n",
    "* Usa truncating='post', lo que significa que si la secuencia es m√°s larga, se corta al final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Aplicar padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aplicar_padding(sequences, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convierte las secuencias tokenizadas en matrices listas para LSTM.\n",
    "* X ahora es una matriz (n√∫mero de ejemplos, 200).\n",
    "* Ejemplo de salida:\n",
    "\n",
    "Antes del padding:\n",
    "[5, 87, 302, 15, 928]\n",
    "\n",
    "Despu√©s del padding:\n",
    "[5, 87, 302, 15, 928, 0, 0, 0, 0, ..., 0]  (hasta 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Imprimir informaci√≥n sobre los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Total de palabras en el vocabulario: 123089\n",
      "üî† Forma de una secuencia de entrada: (14396, 200)\n"
     ]
    }
   ],
   "source": [
    "# Imprimir informaci√≥n sobre los datos tokenizados\n",
    "print(f\"üî¢ Total de palabras en el vocabulario: {len(tokenizer.word_index)}\")\n",
    "print(f\"üî† Forma de una secuencia de entrada: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cantidad de palabras en el vocabulario (tokenizer.word_index).\n",
    "* Dimensiones de las secuencias (X.shape)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Guardar el tokenizer para uso futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar tokenizer para uso futuro\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Guarda el tokenizer en un archivo pickle (.pkl) para reutilizarlo sin volver a tokenizar el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Muestra las primeras 5 secuencias tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "primeras 5 secuencias procesadas:\n",
      "[[ 180 1753 1680 2827 2178 4278  169  889    1 3790    1  201    1 1127\n",
      "   297 1014  114 4666  908  544 1806  108    1  201    1  807    1   10\n",
      "  1074 1402 3870 3628    1  527    1  674    1    1 2990  609  234 1680\n",
      "  2827    1 4278 3790  133 2179  601 1010 1369    1  934    1  608    1\n",
      "  4397    1 1127 1014   10  297    1    1    1    1  149    1 1080 2797\n",
      "   938 3790 2138  113 2959    1 2827    1  149  890  420    1    1  180\n",
      "  1457  133    1 2827    1 1768  489    1 1705 1229    1 4666 1954    1\n",
      "    24  453    1  340  404  640  587  170   90  485 3492   99  103  201\n",
      "     1    1 1464 1052   99    1  242   76    4   11    2   15   10    7\n",
      "     8   16    4   14   93   83    7    8   41  110   77    6    3    2\n",
      "    12   58   64   65   38  160  135  190  244  277  312  125   55   68\n",
      "     2  132  143  141   67  146   55   68    9    2  111  145   91   95\n",
      "   140  112  188    2  133  241  192  187  325  207    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   1 4628    1    1    1    1 2804 4366    1 3432 3408 2614  233  223\n",
      "  3050    1  301    1 1449 1486  279   97  151  685   90    1    1 4338\n",
      "     1  445 2166    1  165 3198  353    1  233  259    1    1  296  418\n",
      "   835 1107 1486  279 2082    1    1  993    1    1 1505    1 3050  353\n",
      "   558  445    1  838    1 1681    1    1  107  423 2648 3514 3408 2570\n",
      "    92  259  233    1    1  353  197  445  871 2818  138    1    1 1350\n",
      "     1 1027  658 3051 2632    1  151  622  459    1 2818 2066 1927 3075\n",
      "     7   82    1 3791  799    1 1807 1155    1  668  748    1  232 1074\n",
      "  2728   71 1928  233 2026 3572  231    1  587    1 1105 3106  231  755\n",
      "   587 1123 3759 2095 1195  353 2943  577    1    1 3572 2520    1  327\n",
      "   280   24  227  771 1088 1068    1    1    1  398  790 3677    1    1\n",
      "  3871 1195    1  616    1 1249 3051  558  328 2246    1  165  333  180\n",
      "  3241  122  176  165  587  576  492    1 3461 1796    1 2729    1 2046\n",
      "     1    1    1 3662  400 2729  276    1 2944  800 2765 1648  362 1442\n",
      "  1468    1    1  283]\n",
      " [ 302 3255  898    1  171   89    1   87 2920    1  319  866  144 1670\n",
      "     1 1241    1  319    1    1 3272 4367    1  843    1    1    1  144\n",
      "     1 2038 3020  275  302 2932 3353  192 3091 2521 1764    1    1  571\n",
      "     1    1    1  223 3010  164   41    1 4751 1132  424  324    1 3272\n",
      "    37 1261 2740  183 2979    1 1505   78    1  346 3629  744  386    1\n",
      "     1  398 2499   44  164    1    1    1 1236    1    1  107    1 3076\n",
      "     1   60   37    1    1    1    1  290 3010 2062    1 1754 1346    1\n",
      "  1435    1    1 3629    1 4461 4558    1   18 1538 1435   88 3433  243\n",
      "   228   89    1    1    1    1    1 1797    1  784  568 2740    1 4197\n",
      "  4629   71    7   82 3629   79 2598 2294    1   37 4920    1   37 1065\n",
      "  2615    1   37 1065 2543  608   37 1065    1    1  470    1  197 1480\n",
      "   103 3462  129    1    1   37  571    1    1  197 1195    1 3272  853\n",
      "  2615  716    1    1    1    1  144 1180 2441    1 1670  136  103 4485\n",
      "   194  501   44    1   71    1 2730  950  150   56 2960    1  810  532\n",
      "   106 2200    1    1]\n",
      " [   1   65  608 2943    1 4253    1  506    1    1  862 2679 3573    1\n",
      "   762    1  645  192  233 1910 1074 1726    1 4873  233 1726    1    1\n",
      "   103 2209    1 1972  165 1080  327    1 4792 1249   98  740  549  144\n",
      "  2598    1  122    1  815 4023    1 4520  233  223 4873   10    1  732\n",
      "  1967  732    3   37   41 4023    1    1    1    1  353  269    1    1\n",
      "    99  311   56    1  887 2359    1    1  151  622    1  576  492    1\n",
      "  3021    1    1  150 1682   37 4874    1 2062  443   41 2114    1  632\n",
      "  1722  367  136  800  215    1  506    1  353 4594  708  136 1632    1\n",
      "     1    1  409 2046  100    1  435  106    1 4630    1    1  507  116\n",
      "  1363    1  506    1 1609    1    1  662  748    1  117   10 1769    1\n",
      "  2500    1    1   71  909    1  269  870    1  353 1702    1    1 1129\n",
      "   836   72    1 1544    1  353  976  587  576  492    1 4399  441  487\n",
      "  1567 1195  506    1 2680    1 2115    1  430   63  199    4   11    2\n",
      "    15   10    7    8   16    4   14   93   83    7    8   41  110   77\n",
      "     6    3    2   12]\n",
      " [   1  928  769  996  761 1443    1   69 2561    1  126 2561    1  928\n",
      "  3872 2252  360    1  513  109  313 4339  216  219    1  152  193  550\n",
      "   358  304 2465    1  266  867  283 2501   78  377  289  780    1  323\n",
      "     1    1 2561   92 2368    1   44   24  425  496 1130 2271    1  733\n",
      "   496 3515  637 1117  496 4072  928  893  513 4667  315  769   72    1\n",
      "  4486   92 2252    1    1  928  761  218  893  928  218    1 1248  299\n",
      "   761  133  295  877  216  474    1    1  688    1 4400  474    1    1\n",
      "   103    1 3077 2252    1    1  160    1  513 1061 1996    1    1    1\n",
      "  2561   92 2368  991 1075    1    1  372  500   37 3409  816  980  206\n",
      "  4714  258 2007  377  266  867 4003 2561    1  167 3130  218 2348 3077\n",
      "  2252 2413    3  360  133  295  648  323 2589    1 2561   92 2368  266\n",
      "   257   37  500  323 2502  816 2681 2561    1  823  514  275    1  286\n",
      "     1 2252    1 1035    1 1489 2027  761 1076    1    1  773  474   81\n",
      "     1  359   37 1544   71    1   60  733  935    1 3732  761    1    1\n",
      "   198  315    1 1208]]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar algunas secuencias tokenizadas\n",
    "print(\"\\nprimeras 5 secuencias procesadas:\")\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cada fila representa un texto convertido en n√∫meros y normalizado para usarlo en LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Definir la funci√≥n obtener_etiquetas_relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_etiquetas_relevantes(data, columna_etiqueta=\"Etiqueta\", top_n=4):\n",
    "    etiquetas_comunes = Counter(data[columna_etiqueta]).most_common(top_n)\n",
    "    etiquetas_relevantes = [etiqueta[0] for etiqueta in etiquetas_comunes]\n",
    "    return etiquetas_relevantes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øQu√© hace esta funci√≥n?\n",
    "\n",
    "* Cuenta la frecuencia de cada etiqueta en la columna \"Etiqueta\" utilizando Counter de collections.\n",
    "* Obtiene las top_n etiquetas m√°s comunes con .most_common(top_n).\n",
    "* top_n=4 significa que se seleccionar√°n las 4 etiquetas m√°s frecuentes.\n",
    "* Extrae solo los nombres de las etiquetas en una lista.\n",
    "* Devuelve la lista de etiquetas m√°s comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Obtener y mostrar las etiquetas m√°s relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Etiquetas m√°s relevantes: ['archivo', 'colombia', 'deportes', 'opinion']\n"
     ]
    }
   ],
   "source": [
    "etiquetas_mas_relevantes = obtener_etiquetas_relevantes(data)\n",
    "print(f\"\\nüéØ Etiquetas m√°s relevantes: {etiquetas_mas_relevantes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se llama a la funci√≥n obtener_etiquetas_relevantes(data).\n",
    "* Se almacenan las etiquetas m√°s comunes en etiquetas_mas_relevantes.\n",
    "* Se imprimen las etiquetas seleccionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Filtrar el dataset para conservar solo esas etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Nuevo tama√±o del dataset despu√©s del filtro: (11380, 8)\n"
     ]
    }
   ],
   "source": [
    "data_filtrada = data[data[\"Etiqueta\"].isin(etiquetas_mas_relevantes)]\n",
    "print(f\"\\nüìå Nuevo tama√±o del dataset despu√©s del filtro: {data_filtrada.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esto indica que, despu√©s del filtrado, el dataset ahora tiene 11380 filas y 8 columnas."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
